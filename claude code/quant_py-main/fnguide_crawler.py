"""
FnGuide Company Guide ì¬ë¬´ì œí‘œ í¬ë¡¤ë§ ëª¨ë“ˆ
ê¸°ì¡´ ì±…ì˜ ì½”ë“œë¥¼ ê¸°ë°˜ìœ¼ë¡œ êµ¬í˜„

TODO Task #9: ìƒì¡´ í¸í–¥ ì œê±°
- ê³¼ê±° ìƒì¥íì§€ ì¢…ëª© DB êµ¬ì¶•
- ìƒì¥íì§€ ì‹œì ì˜ ì†ì‹¤ ë°˜ì˜ (-100% ë˜ëŠ” ìµœì¢… ê±°ë˜ê°€)
- ë°ì´í„° ì†ŒìŠ¤: KRX ìƒì¥íì§€ ì¢…ëª© ë¦¬ìŠ¤íŠ¸
- êµ¬í˜„ ìš°ì„ ìˆœìœ„: ì¤‘ê°„ (ë°±í…ŒìŠ¤íŠ¸ ì •í™•ë„ í–¥ìƒ)
"""

import pandas as pd
import numpy as np
import requests as rq
from bs4 import BeautifulSoup
import re
import time
from pathlib import Path
from tqdm import tqdm

# ë°ì´í„° ìºì‹± ë””ë ‰í† ë¦¬
DATA_DIR = Path(__file__).parent / 'data_cache'
DATA_DIR.mkdir(exist_ok=True)


def clean_fs(df, ticker, frequency):
    """
    ì¬ë¬´ì œí‘œ ë°ì´í„°ë¥¼ ì •ê·œí™”í•˜ëŠ” í•¨ìˆ˜

    Parameters:
    - df: ì›ë³¸ ì¬ë¬´ì œí‘œ ë°ì´í„°í”„ë ˆì„
    - ticker: ì¢…ëª© ì½”ë“œ
    - frequency: ê³µì‹œêµ¬ë¶„ ('y' for ì—°ê°„, 'q' for ë¶„ê¸°)

    Returns:
    - ì •ê·œí™”ëœ ë°ì´í„°í”„ë ˆì„
    """
    # ëª¨ë“  ë°ì´í„°ê°€ NaNì¸ í–‰ ì œê±°
    df = df[~df.loc[:, ~df.columns.isin(['ê³„ì •'])].isna().all(axis=1)]

    # ê³„ì •ëª… ì¤‘ë³µ ì œê±° (ì²« ë²ˆì§¸ ê°’ë§Œ ìœ ì§€)
    df = df.drop_duplicates(['ê³„ì •'], keep='first')

    # Wide formatì„ Long formatìœ¼ë¡œ ë³€í™˜
    df = pd.melt(df, id_vars='ê³„ì •', var_name='ê¸°ì¤€ì¼', value_name='ê°’')

    # ê°’ì´ Nullì¸ í–‰ ì œê±°
    df = df[~pd.isnull(df['ê°’'])]

    # '[+]' ë²„íŠ¼ì— í•´ë‹¹í•˜ëŠ” 'ê³„ì‚°ì— ì°¸ì—¬í•œ ê³„ì • í¼ì¹˜ê¸°' í…ìŠ¤íŠ¸ ì œê±°
    df['ê³„ì •'] = df['ê³„ì •'].replace({'ê³„ì‚°ì— ì°¸ì—¬í•œ ê³„ì • í¼ì¹˜ê¸°': ''}, regex=True)

    # ê¸°ì¤€ì¼ì„ ì›”ë§ ë‚ ì§œë¡œ ë³€í™˜
    df['ê¸°ì¤€ì¼'] = pd.to_datetime(df['ê¸°ì¤€ì¼'],
                               format='%Y/%m') + pd.tseries.offsets.MonthEnd()

    # ì¢…ëª©ì½”ë“œì™€ ê³µì‹œêµ¬ë¶„ ì¶”ê°€
    df['ì¢…ëª©ì½”ë“œ'] = ticker
    df['ê³µì‹œêµ¬ë¶„'] = frequency

    return df


def get_financial_statement(ticker, use_cache=True):
    """
    FnGuideì—ì„œ íŠ¹ì • ì¢…ëª©ì˜ ì¬ë¬´ì œí‘œ í¬ë¡¤ë§

    Args:
        ticker: 6ìë¦¬ ì¢…ëª©ì½”ë“œ
        use_cache: ìºì‹œ ì‚¬ìš© ì—¬ë¶€

    Returns:
        dict: {'annual': ì—°ê°„ ë°ì´í„°, 'quarter': ë¶„ê¸° ë°ì´í„°}
    """
    cache_file = DATA_DIR / f'fs_fnguide_{ticker}.parquet'

    if use_cache and cache_file.exists():
        print(f"ìºì‹œì—ì„œ ì¬ë¬´ì œí‘œ ë¡œë“œ: {ticker}")
        return pd.read_parquet(cache_file)

    try:
        # URL ìƒì„±
        url = f'http://comp.fnguide.com/SVO2/ASP/SVD_Finance.asp?pGB=1&gicode=A{ticker}'

        # ë°ì´í„° ë°›ì•„ì˜¤ê¸° (HTML í…Œì´ë¸” íŒŒì‹±)
        data = pd.read_html(url, displayed_only=False, encoding='utf-8')

        # ========== ì—°ê°„(Annual) ì¬ë¬´ì œí‘œ ì²˜ë¦¬ ==========
        # data[0]: í¬ê´„ì†ìµê³„ì‚°ì„œ (ì—°ê°„)
        # data[2]: ì¬ë¬´ìƒíƒœí‘œ (ì—°ê°„)
        # data[4]: í˜„ê¸ˆíë¦„í‘œ (ì—°ê°„)

        # 'ì „ë…„ë™ê¸°' ì—´ ì œì™¸ í›„ ì„¸ í…Œì´ë¸” ë³‘í•©
        data_fs_y = pd.concat([
            data[0].iloc[:, ~data[0].columns.str.contains('ì „ë…„ë™ê¸°')],
            data[2],
            data[4]
        ])
        data_fs_y = data_fs_y.rename(columns={data_fs_y.columns[0]: "ê³„ì •"})

        # ê²°ì‚°ë…„(fiscal year) ì°¾ê¸°
        page_data = rq.get(url)
        page_data_html = BeautifulSoup(page_data.content, 'html.parser')

        fiscal_data = page_data_html.select('div.corp_group1 > h2')
        if len(fiscal_data) > 1:
            fiscal_data_text = fiscal_data[1].text
            fiscal_data_text = re.findall('[0-9]+', fiscal_data_text)

            # ê²°ì‚°ë…„ì— í•´ë‹¹í•˜ëŠ” ê³„ì •ë§Œ ë‚¨ê¸°ê¸°
            data_fs_y = data_fs_y.loc[:, (data_fs_y.columns == 'ê³„ì •') | (
                data_fs_y.columns.str[-2:].isin(fiscal_data_text))]

        # ì—°ê°„ ë°ì´í„° í´ë Œì§•
        data_fs_y_clean = clean_fs(data_fs_y, ticker, 'y')

        # ========== ë¶„ê¸°(Quarterly) ì¬ë¬´ì œí‘œ ì²˜ë¦¬ ==========
        # data[1]: í¬ê´„ì†ìµê³„ì‚°ì„œ (ë¶„ê¸°)
        # data[3]: ì¬ë¬´ìƒíƒœí‘œ (ë¶„ê¸°)
        # data[5]: í˜„ê¸ˆíë¦„í‘œ (ë¶„ê¸°)

        # 'ì „ë…„ë™ê¸°' ì—´ ì œì™¸ í›„ ì„¸ í…Œì´ë¸” ë³‘í•©
        data_fs_q = pd.concat([
            data[1].iloc[:, ~data[1].columns.str.contains('ì „ë…„ë™ê¸°')],
            data[3],
            data[5]
        ])
        data_fs_q = data_fs_q.rename(columns={data_fs_q.columns[0]: "ê³„ì •"})

        # ë¶„ê¸° ë°ì´í„° í´ë Œì§•
        data_fs_q_clean = clean_fs(data_fs_q, ticker, 'q')

        # ì—°ê°„ê³¼ ë¶„ê¸° ë°ì´í„° í†µí•©
        data_fs_bind = pd.concat([data_fs_y_clean, data_fs_q_clean])

        # ìºì‹œ ì €ì¥
        data_fs_bind.to_parquet(cache_file)

        time.sleep(1)  # í¬ë¡¤ë§ ì˜ˆì˜ (ìºì‹œ íˆíŠ¸ ì‹œ ê±´ë„ˆëœ€)

        return data_fs_bind

    except Exception as e:
        print(f"ì¢…ëª© {ticker} ì¬ë¬´ì œí‘œ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        return pd.DataFrame()


def get_all_financial_statements(tickers, use_cache=True):
    """
    ì—¬ëŸ¬ ì¢…ëª©ì˜ ì¬ë¬´ì œí‘œ í¬ë¡¤ë§

    Args:
        tickers: ì¢…ëª© ë¦¬ìŠ¤íŠ¸
        use_cache: ìºì‹œ ì‚¬ìš© ì—¬ë¶€

    Returns:
        dict: {ticker: ì¬ë¬´ì œí‘œ ë°ì´í„°í”„ë ˆì„}
    """
    print(f"FnGuideì—ì„œ ì¬ë¬´ì œí‘œ ìˆ˜ì§‘ ì¤‘ (ì´ {len(tickers)}ê°œ ì¢…ëª©)...")

    fs_data = {}
    error_list = []

    for ticker in tqdm(tickers):
        try:
            fs = get_financial_statement(ticker, use_cache=use_cache)
            if not fs.empty:
                fs_data[ticker] = fs
        except Exception as e:
            print(f"\nì¢…ëª© {ticker} ì‹¤íŒ¨: {e}")
            error_list.append(ticker)
            continue

    print(f"\nìˆ˜ì§‘ ì™„ë£Œ: {len(fs_data)}ê°œ ì„±ê³µ, {len(error_list)}ê°œ ì‹¤íŒ¨")
    if error_list:
        print(f"ì‹¤íŒ¨ ì¢…ëª©: {error_list[:10]}...")

    return fs_data


def extract_magic_formula_data(fs_dict, base_date=None, use_ttm=True):
    """
    ì¬ë¬´ì œí‘œì—ì„œ ë§ˆë²•ê³µì‹ ê³„ì‚°ì— í•„ìš”í•œ í•­ëª© ì¶”ì¶œ

    í•„ìš” í•­ëª©:
    - ë‹¹ê¸°ìˆœì´ìµ, ë²•ì¸ì„¸ë¹„ìš©, ì´ìë¹„ìš©
    - ìì‚°, ë¶€ì±„, ìœ ë™ë¶€ì±„, ìœ ë™ìì‚°, ë¹„ìœ ë™ìì‚°
    - í˜„ê¸ˆë°í˜„ê¸ˆì„±ìì‚°
    - ê°ê°€ìƒê°ë¹„

    Args:
        fs_dict: {ticker: ì¬ë¬´ì œí‘œ ë°ì´í„°í”„ë ˆì„}
        base_date: ê¸°ì¤€ì¼ (str, YYYYMMDD). Noneì´ë©´ ìµœì‹  ë°ì´í„° ì‚¬ìš©.
                   ê³µì‹œ ì‹œì°¨ ë°˜ì˜: ë¶„ê¸°ëŠ” 45ì¼, ì—°ê°„ì€ 90ì¼
        use_ttm: Trueë©´ TTM(ìµœê·¼ 4ë¶„ê¸° í•©ì‚°) ì‚¬ìš©, Falseë©´ ì—°ê°„ ë°ì´í„°ë§Œ ì‚¬ìš©

    Returns:
        ë°ì´í„°í”„ë ˆì„: ì¢…ëª©ë³„ TTM ë˜ëŠ” ì—°ê°„ ì¬ë¬´ì œí‘œ í•­ëª©
    """
    result_list = []

    # ì†ìµê³„ì‚°ì„œ/í˜„ê¸ˆíë¦„í‘œ í•­ëª© (4ë¶„ê¸° í•©ì‚° ëŒ€ìƒ)
    flow_accounts = [
        'ë‹¹ê¸°ìˆœì´ìµ', 'ë²•ì¸ì„¸ë¹„ìš©', 'ì„¸ì „ê³„ì†ì‚¬ì—…ì´ìµ',
        'ë§¤ì¶œì•¡', 'ë§¤ì¶œì´ì´ìµ', 'ì˜ì—…ì´ìµ',
        'ì˜ì—…í™œë™ìœ¼ë¡œì¸í•œí˜„ê¸ˆíë¦„', 'ê°ê°€ìƒê°ë¹„'
    ]

    # ì¬ë¬´ìƒíƒœí‘œ í•­ëª© (ìµœê·¼ ë¶„ê¸° ê°’ ì‚¬ìš© - ìŠ¤ëƒ…ìƒ·)
    stock_accounts = [
        'ìì‚°', 'ë¶€ì±„', 'ìœ ë™ë¶€ì±„', 'ìœ ë™ìì‚°', 'ë¹„ìœ ë™ìì‚°',
        'í˜„ê¸ˆë°í˜„ê¸ˆì„±ìì‚°', 'ìë³¸'
    ]

    # ê¸°ì¤€ì¼ ì„¤ì • (ê³µì‹œ ì‹œì°¨ ë°˜ì˜)
    if base_date:
        from datetime import datetime, timedelta
        base_dt = datetime.strptime(base_date, '%Y%m%d')
        # ë¶„ê¸° ê³µì‹œ ì‹œì°¨: 45ì¼, ì—°ê°„ ê³µì‹œ ì‹œì°¨: 90ì¼
        cutoff_date_quarterly = base_dt - timedelta(days=45)
        cutoff_date_annual = base_dt - timedelta(days=90)
    else:
        cutoff_date_quarterly = None
        cutoff_date_annual = None

    for ticker, fs_df in fs_dict.items():
        if use_ttm:
            # TTM ë°©ì‹: ë¶„ê¸° ë°ì´í„° ì‚¬ìš©
            quarterly_data = fs_df[fs_df['ê³µì‹œêµ¬ë¶„'] == 'q'].copy()

            if quarterly_data.empty:
                # ë¶„ê¸° ë°ì´í„° ì—†ìœ¼ë©´ ì—°ê°„ ë°ì´í„°ë¡œ í´ë°±
                annual_data = fs_df[fs_df['ê³µì‹œêµ¬ë¶„'] == 'y'].copy()
                if cutoff_date_annual:
                    annual_data = annual_data[annual_data['ê¸°ì¤€ì¼'] <= cutoff_date_annual]
                if annual_data.empty:
                    continue
                latest_date = annual_data['ê¸°ì¤€ì¼'].max()
                latest_data = annual_data[annual_data['ê¸°ì¤€ì¼'] == latest_date]
                pivot_data = latest_data.pivot_table(
                    index='ì¢…ëª©ì½”ë“œ', columns='ê³„ì •', values='ê°’', aggfunc='first'
                )
                pivot_data['ê¸°ì¤€ì¼'] = latest_date
                result_list.append(pivot_data)
                continue

            # ê³µì‹œ ì‹œì°¨ ë°˜ì˜
            if cutoff_date_quarterly:
                quarterly_data = quarterly_data[quarterly_data['ê¸°ì¤€ì¼'] <= cutoff_date_quarterly]

            if quarterly_data.empty:
                continue

            # ìµœê·¼ 4ë¶„ê¸° ì°¾ê¸°
            unique_dates = sorted(quarterly_data['ê¸°ì¤€ì¼'].unique(), reverse=True)
            if len(unique_dates) < 4:
                # 4ë¶„ê¸° ë¯¸ë§Œì´ë©´ ìˆëŠ” ë§Œí¼ë§Œ ì‚¬ìš© (ë¹„ìœ¨ ì¡°ì •)
                recent_dates = unique_dates
            else:
                recent_dates = unique_dates[:4]

            latest_date = recent_dates[0]  # ê°€ì¥ ìµœê·¼ ë¶„ê¸°

            # ìµœê·¼ 4ë¶„ê¸° ë°ì´í„° ì¶”ì¶œ
            ttm_data = quarterly_data[quarterly_data['ê¸°ì¤€ì¼'].isin(recent_dates)]

            # ì†ìµê³„ì‚°ì„œ/í˜„ê¸ˆíë¦„í‘œ: ê°€ì¤‘ TTM (ìµœì‹ ë¶„ê¸° ê°€ì¤‘ì¹˜ ë†’ìŒ)
            # ê°€ì¤‘ì¹˜: ìµœì‹  40%, 2ë²ˆì§¸ 30%, 3ë²ˆì§¸ 20%, 4ë²ˆì§¸ 10%
            # í•©=4 ìŠ¤ì¼€ì¼(1.6/1.2/0.8/0.4)ë¡œ ê¸°ì¡´ TTM í•©ì‚°ê³¼ ë™ì¼ ìŠ¤ì¼€ì¼ ìœ ì§€
            # 4ë¶„ê¸° ë¯¸ë§Œ ì‹œ í•©ì´ 4.0ì´ ë˜ë„ë¡ ì •ê·œí™”
            base_weights = [1.6, 1.2, 0.8, 0.4]  # ìµœì‹ â†’ê³¼ê±° ìˆœ
            n_quarters = len(recent_dates)
            raw_weights = base_weights[:n_quarters]
            scale = 4.0 / sum(raw_weights)  # í•©ì´ 4.0ì´ ë˜ë„ë¡ ì •ê·œí™”

            weight_map = {}
            for i, d in enumerate(sorted(recent_dates, reverse=True)):
                weight_map[d] = raw_weights[i] * scale if i < n_quarters else base_weights[-1]

            flow_data = ttm_data[ttm_data['ê³„ì •'].isin(flow_accounts)].copy()
            flow_data['ê°€ì¤‘ì¹˜'] = flow_data['ê¸°ì¤€ì¼'].map(weight_map)
            flow_data['ê°€ì¤‘ê°’'] = flow_data['ê°’'] * flow_data['ê°€ì¤‘ì¹˜']
            flow_sum = flow_data.groupby(['ì¢…ëª©ì½”ë“œ', 'ê³„ì •'])['ê°€ì¤‘ê°’'].sum().reset_index()
            flow_sum = flow_sum.rename(columns={'ê°€ì¤‘ê°’': 'ê°’'})
            flow_pivot = flow_sum.pivot_table(
                index='ì¢…ëª©ì½”ë“œ', columns='ê³„ì •', values='ê°’', aggfunc='first'
            )

            # ì¬ë¬´ìƒíƒœí‘œ: ìµœê·¼ ë¶„ê¸° ê°’
            stock_data = ttm_data[
                (ttm_data['ê³„ì •'].isin(stock_accounts)) &
                (ttm_data['ê¸°ì¤€ì¼'] == latest_date)
            ]
            stock_pivot = stock_data.pivot_table(
                index='ì¢…ëª©ì½”ë“œ', columns='ê³„ì •', values='ê°’', aggfunc='first'
            )

            # í•©ì¹˜ê¸°
            if flow_pivot.empty and stock_pivot.empty:
                continue

            pivot_data = pd.concat([flow_pivot, stock_pivot], axis=1)
            pivot_data['ê¸°ì¤€ì¼'] = latest_date
            result_list.append(pivot_data)

        else:
            # ê¸°ì¡´ ë°©ì‹: ì—°ê°„ ë°ì´í„°ë§Œ ì‚¬ìš©
            annual_data = fs_df[fs_df['ê³µì‹œêµ¬ë¶„'] == 'y'].copy()

            if annual_data.empty:
                continue

            if cutoff_date_annual:
                annual_data = annual_data[annual_data['ê¸°ì¤€ì¼'] <= cutoff_date_annual]

            if annual_data.empty:
                continue

            latest_date = annual_data['ê¸°ì¤€ì¼'].max()
            latest_data = annual_data[annual_data['ê¸°ì¤€ì¼'] == latest_date]

            pivot_data = latest_data.pivot_table(
                index='ì¢…ëª©ì½”ë“œ', columns='ê³„ì •', values='ê°’', aggfunc='first'
            )

            pivot_data['ê¸°ì¤€ì¼'] = latest_date
            result_list.append(pivot_data)

    if not result_list:
        return pd.DataFrame()

    # ì „ì²´ ê²°í•©
    result_df = pd.concat(result_list)
    result_df = result_df.reset_index()

    # ì£¼ìš” ê³„ì •ëª… ë§¤í•‘ (FnGuide ì‹¤ì œ ê³„ì •ëª… í™•ì¸ ì™„ë£Œ)
    # â€» 'ì´ìë¹„ìš©'ì€ FnGuideì— ì—†ìœ¼ë¯€ë¡œ ëŒ€ì•ˆ ì‚¬ìš©
    account_mapping = {
        'ë‹¹ê¸°ìˆœì´ìµ': 'ë‹¹ê¸°ìˆœì´ìµ',
        'ì„¸ì „ê³„ì†ì‚¬ì—…ì´ìµ': 'ì„¸ì „ê³„ì†ì‚¬ì—…ì´ìµ',  # EBIT ê³„ì‚°ìš©
        'ë²•ì¸ì„¸ë¹„ìš©': 'ë²•ì¸ì„¸ë¹„ìš©',
        'ìì‚°': 'ìì‚°',
        'ë¶€ì±„': 'ì´ë¶€ì±„',  # 'ë¶€ì±„'ë¥¼ 'ì´ë¶€ì±„'ë¡œ ë§¤í•‘
        'ìœ ë™ë¶€ì±„': 'ìœ ë™ë¶€ì±„',
        'ìœ ë™ìì‚°': 'ìœ ë™ìì‚°',
        'ë¹„ìœ ë™ìì‚°': 'ë¹„ìœ ë™ìì‚°',
        'í˜„ê¸ˆë°í˜„ê¸ˆì„±ìì‚°': 'í˜„ê¸ˆ',
        'ê°ê°€ìƒê°ë¹„': 'ê°ê°€ìƒê°ë¹„',
        'ìë³¸': 'ìë³¸',
        'ë§¤ì¶œì•¡': 'ë§¤ì¶œì•¡',
        'ë§¤ì¶œì´ì´ìµ': 'ë§¤ì¶œì´ì´ìµ',
        'ì˜ì—…í™œë™ìœ¼ë¡œì¸í•œí˜„ê¸ˆíë¦„': 'ì˜ì—…í˜„ê¸ˆíë¦„',
        'ì˜ì—…ì´ìµ': 'ì˜ì—…ì´ìµ',
    }

    # ì»¬ëŸ¼ëª… ë³€ê²½ (ì›ë³¸ ê³„ì •ëª… â†’ ê°„ì†Œí™”ëœ ì´ë¦„)
    rename_dict = {}
    for original_name, simple_name in account_mapping.items():
        if original_name in result_df.columns and original_name != simple_name:
            rename_dict[original_name] = simple_name

    if rename_dict:
        result_df = result_df.rename(columns=rename_dict)

    # ê°€ëŠ¥í•œ ì»¬ëŸ¼ë§Œ ì„ íƒ
    available_cols = ['ì¢…ëª©ì½”ë“œ', 'ê¸°ì¤€ì¼']
    for simple_name in account_mapping.values():
        if simple_name in result_df.columns:
            available_cols.append(simple_name)

    result_df = result_df[[col for col in available_cols if col in result_df.columns]]

    return result_df


def extract_revenue_growth(fs_dict, base_date=None):
    """
    ì¬ë¬´ì œí‘œì—ì„œ ë§¤ì¶œì„±ì¥ë¥ (YoY) ì¶”ì¶œ

    ì—°ê°„ ë§¤ì¶œì•¡ 2ê°œë…„ ë¹„êµ:
      ë§¤ì¶œì„±ì¥ë¥  = (ìµœê·¼ë§¤ì¶œì•¡ - ì „ê¸°ë§¤ì¶œì•¡) / |ì „ê¸°ë§¤ì¶œì•¡| Ã— 100

    Args:
        fs_dict: {ticker: ì¬ë¬´ì œí‘œ ë°ì´í„°í”„ë ˆì„}
        base_date: ê¸°ì¤€ì¼ (str, YYYYMMDD)

    Returns:
        DataFrame: ì¢…ëª©ì½”ë“œ, ë§¤ì¶œì„±ì¥ë¥ 
    """
    if base_date:
        from datetime import datetime, timedelta
        base_dt = datetime.strptime(base_date, '%Y%m%d')
        cutoff_date = base_dt - timedelta(days=90)  # ì—°ê°„ ê³µì‹œ ì‹œì°¨
    else:
        cutoff_date = None

    results = []

    for ticker, fs_df in fs_dict.items():
        annual = fs_df[(fs_df['ê³µì‹œêµ¬ë¶„'] == 'y') & (fs_df['ê³„ì •'] == 'ë§¤ì¶œì•¡')].copy()

        if cutoff_date is not None:
            annual = annual[annual['ê¸°ì¤€ì¼'] <= cutoff_date]

        if len(annual) < 2:
            continue

        annual = annual.sort_values('ê¸°ì¤€ì¼', ascending=False)
        latest_rev = annual.iloc[0]['ê°’']
        prior_rev = annual.iloc[1]['ê°’']

        if pd.isna(prior_rev) or prior_rev == 0:
            continue

        yoy = (latest_rev - prior_rev) / abs(prior_rev) * 100
        results.append({'ì¢…ëª©ì½”ë“œ': ticker, 'ë§¤ì¶œì„±ì¥ë¥ ': yoy})

    if not results:
        return pd.DataFrame(columns=['ì¢…ëª©ì½”ë“œ', 'ë§¤ì¶œì„±ì¥ë¥ '])

    df = pd.DataFrame(results)
    valid = df['ë§¤ì¶œì„±ì¥ë¥ '].notna().sum()
    print(f"ë§¤ì¶œì„±ì¥ë¥ (YoY) ê³„ì‚°: {valid}/{len(fs_dict)}ê°œ ì¢…ëª© (ì—°ê°„ ë§¤ì¶œì•¡ 2ê°œë…„ ë¹„êµ)")
    return df


# ============================================================================
# ì»¨ì„¼ì„œìŠ¤ ë°ì´í„° í¬ë¡¤ë§ (Forward EPS/PER)
# ============================================================================

def get_consensus_data(ticker):
    """
    FnGuide ë©”ì¸ í˜ì´ì§€ì—ì„œ ì»¨ì„¼ì„œìŠ¤ ë°ì´í„° ì¶”ì¶œ

    URL: comp.fnguide.com/SVO2/ASP/SVD_Main.asp?gicode=A{ticker}
    í…Œì´ë¸”: íˆ¬ìì˜ê²¬ / ì»¨ì„¼ì„œìŠ¤ ìš”ì•½

    Returns:
        dict: forward_eps, forward_per, analyst_count, target_price ë“±
    """
    result = {
        'ticker': ticker,
        'forward_eps': None,
        'forward_per': None,
        'analyst_count': None,
        'target_price': None,
        'has_consensus': False,
    }

    try:
        url = f'http://comp.fnguide.com/SVO2/ASP/SVD_Main.asp?pGB=1&gicode=A{ticker}'

        # HTML í…Œì´ë¸” íŒŒì‹±
        tables = pd.read_html(url, displayed_only=False, encoding='utf-8')

        # ì»¨ì„¼ì„œìŠ¤ í…Œì´ë¸” ì°¾ê¸° (ë³´í†µ ì¸ë±ìŠ¤ 7~10 ì‚¬ì´)
        for i, tbl in enumerate(tables):
            tbl_str = str(tbl.columns.tolist()) + str(tbl.values.tolist())

            # EPS, PER ì»¬ëŸ¼ì´ ìˆëŠ” í…Œì´ë¸” ì°¾ê¸°
            if 'EPS' in tbl_str and 'PER' in tbl_str:
                # EPS ì¶”ì¶œ
                if 'EPS' in tbl.columns:
                    try:
                        eps_val = tbl['EPS'].iloc[0]
                        eps_str = str(eps_val).replace(',', '').replace('ì›', '').strip()
                        if eps_str and eps_str not in ['nan', '-', '']:
                            result['forward_eps'] = float(eps_str)
                            result['has_consensus'] = True
                    except Exception:
                        pass

                # PER ì¶”ì¶œ
                if 'PER' in tbl.columns:
                    try:
                        per_val = tbl['PER'].iloc[0]
                        per_str = str(per_val).replace('ë°°', '').strip()
                        if per_str and per_str not in ['nan', '-', '']:
                            result['forward_per'] = float(per_str)
                    except Exception:
                        pass

                # ëª©í‘œì£¼ê°€ ì¶”ì¶œ
                for col in tbl.columns:
                    if 'ëª©í‘œ' in str(col):
                        try:
                            target_val = tbl[col].iloc[0]
                            target_str = str(target_val).replace(',', '').replace('ì›', '').strip()
                            if target_str and target_str not in ['nan', '-', '']:
                                result['target_price'] = float(target_str)
                        except Exception:
                            pass
                        break

                break

        # ì• ë„ë¦¬ìŠ¤íŠ¸ ìˆ˜ ì¶”ì¶œ ì‹œë„
        if result['has_consensus'] and result['analyst_count'] is None:
            result['analyst_count'] = 1  # ê¸°ë³¸ê°’

    except Exception as e:
        pass

    return result


def get_consensus_batch(tickers, delay=1.0):
    """
    ì—¬ëŸ¬ ì¢…ëª©ì˜ ì»¨ì„¼ì„œìŠ¤ ë°ì´í„° ì¼ê´„ ìˆ˜ì§‘

    Args:
        tickers: ì¢…ëª©ì½”ë“œ ë¦¬ìŠ¤íŠ¸
        delay: ìš”ì²­ ê°„ ë”œë ˆì´ (ì´ˆ)

    Returns:
        pd.DataFrame: ì»¨ì„¼ì„œìŠ¤ ë°ì´í„°
    """
    results = []

    print(f"\nğŸ“Š ì»¨ì„¼ì„œìŠ¤ ë°ì´í„° ìˆ˜ì§‘ ì¤‘... ({len(tickers)}ê°œ ì¢…ëª©)")

    for i, ticker in enumerate(tickers):
        try:
            data = get_consensus_data(ticker)
            results.append(data)

            if (i + 1) % 20 == 0:
                print(f"   {i + 1}/{len(tickers)} ì™„ë£Œ...")

            time.sleep(delay)

        except Exception as e:
            results.append({'ticker': ticker, 'has_consensus': False})

    df = pd.DataFrame(results)

    # ì»¤ë²„ë¦¬ì§€ í†µê³„
    coverage = df['has_consensus'].sum()
    print(f"\nâœ… ì»¨ì„¼ì„œìŠ¤ ì»¤ë²„ë¦¬ì§€: {coverage}/{len(tickers)} ({coverage/len(tickers)*100:.1f}%)")

    return df


# ============================================================================
# ë©”ì¸ í…ŒìŠ¤íŠ¸
# ============================================================================

if __name__ == '__main__':
    # í…ŒìŠ¤íŠ¸
    print("FnGuide í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸")

    # ì‚¼ì„±ì „ì ì¬ë¬´ì œí‘œ ìˆ˜ì§‘
    test_ticker = '005930'
    print(f"\n{test_ticker} ì¬ë¬´ì œí‘œ ìˆ˜ì§‘ ì¤‘...")

    fs_data = get_financial_statement(test_ticker, use_cache=False)

    if not fs_data.empty:
        print(f"\nìˆ˜ì§‘ëœ ë°ì´í„° í¬ê¸°: {fs_data.shape}")
        print(f"\nê³„ì • ëª©ë¡ (ì¼ë¶€):")
        print(fs_data['ê³„ì •'].unique()[:20])

        print(f"\nìµœê·¼ ì—°ê°„ ë°ì´í„°:")
        annual_latest = fs_data[(fs_data['ê³µì‹œêµ¬ë¶„'] == 'y') &
                               (fs_data['ê¸°ì¤€ì¼'] == fs_data['ê¸°ì¤€ì¼'].max())]
        print(annual_latest.head(20))

        # ë§ˆë²•ê³µì‹ ë°ì´í„° ì¶”ì¶œ í…ŒìŠ¤íŠ¸
        print(f"\në§ˆë²•ê³µì‹ ë°ì´í„° ì¶”ì¶œ í…ŒìŠ¤íŠ¸:")
        magic_data = extract_magic_formula_data({test_ticker: fs_data})
        print(magic_data)

    # ì»¨ì„¼ì„œìŠ¤ ë°ì´í„° í…ŒìŠ¤íŠ¸
    print(f"\n\n{'='*70}")
    print("ì»¨ì„¼ì„œìŠ¤ ë°ì´í„° í…ŒìŠ¤íŠ¸")
    print('='*70)

    test_tickers = ['005930', '018290', '419530']  # ì‚¼ì„±ì „ì, ë¸Œì´í‹°, SAMGì—”í„°

    for ticker in test_tickers:
        consensus = get_consensus_data(ticker)
        print(f"\n{ticker}:")
        print(f"  Forward EPS: {consensus.get('forward_eps')}")
        print(f"  Forward PER: {consensus.get('forward_per')}")
        print(f"  ëª©í‘œì£¼ê°€: {consensus.get('target_price')}")
        print(f"  ì»¤ë²„ë¦¬ì§€: {consensus.get('has_consensus')}")
        time.sleep(1)
